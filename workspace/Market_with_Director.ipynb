{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "liquid-jacket",
   "metadata": {},
   "source": [
    "# Federated Market with Director example\n",
    "## Using low-level Python API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0579f8",
   "metadata": {},
   "source": [
    "# Long-Living entities update\n",
    "\n",
    "* We now may have director running on another machine.\n",
    "* We use Federation API to communicate with Director.\n",
    "* Federation object should hold a Director's client (for user service)\n",
    "* Keeping in mind that several API instances may be connacted to one Director.\n",
    "\n",
    "\n",
    "* We do not think for now how we start a Director.\n",
    "* But it knows the data shape and target shape for the DataScience problem in the Federation.\n",
    "* Director holds the list of connected envoys, we do not need to specify it anymore.\n",
    "* Director and Envoys are responsible for encrypting connections, we do not need to worry about certs.\n",
    "\n",
    "\n",
    "* Yet we MUST have a cert to communicate to the Director.\n",
    "* We MUST know the FQDN of a Director.\n",
    "* Director communicates data and target shape to the Federation interface object.\n",
    "\n",
    "\n",
    "* Experiment API may use this info to construct a dummy dataset and a `shard descriptor` stub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alike-sharing",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Install dependencies if not already installed\n",
    "!pip install torch==1.9.0\n",
    "!pip install torchvision==0.10.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16986f22",
   "metadata": {},
   "source": [
    "# Connect to the Federation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4485ac79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a federation\n",
    "from openfl.interface.interactive_api.federation import Federation\n",
    "\n",
    "# please use the same identificator that was used in signed certificate\n",
    "cliend_id = 'frontend'\n",
    "\n",
    "# 1) Run with API layer - Director mTLS \n",
    "# If the user wants to enable mTLS their must provide CA root chain, and signed key pair to the federation interface\n",
    "# cert_chain = 'cert/root_ca.crt'\n",
    "# API_certificate = 'cert/frontend.crt'\n",
    "# API_private_key = 'cert/frontend.key'\n",
    "\n",
    "# federation = Federation(client_id='frontend', director_node_fqdn='localhost', director_port='50051', disable_tls=False,\n",
    "#                        cert_chain=cert_chain, api_cert=API_certificate, api_private_key=API_private_key)\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# 2) Run with TLS disabled (trusted environment)\n",
    "# Federation can also determine local fqdn automatically\n",
    "federation = Federation(client_id='frontend', director_node_fqdn='localhost', director_port='50051', disable_tls=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35802d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "shard_registry = federation.get_shard_registry()\n",
    "shard_registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ae50de",
   "metadata": {},
   "outputs": [],
   "source": [
    "federation.target_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920216d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# First, request a dummy_shard_desc that holds information about the federated dataset \n",
    "dummy_shard_desc = federation.get_dummy_shard_descriptor(size=10)\n",
    "sample, target = dummy_shard_desc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "obvious-tyler",
   "metadata": {},
   "source": [
    "## Creating a FL experiment using Interactive API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rubber-address",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openfl.interface.interactive_api.experiment import TaskInterface, DataInterface, ModelInterface, FLExperiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustainable-public",
   "metadata": {},
   "source": [
    "### Register dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlike-texas",
   "metadata": {},
   "source": [
    "We extract User dataset class implementation.\n",
    "Is it convinient?\n",
    "What if the dataset is not a class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64f37dcf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DataInterface' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_2868/3121082169.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[1;31m# Now you can implement you data loaders using dummy_shard_desc\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 8\u001B[1;33m \u001B[1;32mclass\u001B[0m \u001B[0mMarketSD\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mDataInterface\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mDataset\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      9\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     10\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m__init__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'DataInterface' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from dataset import Market1501\n",
    "from tools import ImageDataset, RandomIdentitySampler\n",
    "import transforms as T\n",
    "\n",
    "# Now you can implement you data loaders using dummy_shard_desc\n",
    "class MarketSD(DataInterface, Dataset):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        if self.kwargs['data_path'].isdigit():    # split aggregator data by data path (index 1 or 2 data[index-1::2])\n",
    "            split_data = True\n",
    "        else:    # absolute path\n",
    "            split_data = False\n",
    "\n",
    "        self.dataset = Market1501(root=self.kwargs['data_path'], split_data=split_data)\n",
    "\n",
    "        # Prepare transforms\n",
    "        self.transform_train = T.Compose([\n",
    "            T.RandomCroping(256, 128, p=0.5),\n",
    "            T.RandomHorizontalFlip(),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            T.RandomErasing(probability=0.5)\n",
    "        ])\n",
    "        self.transform_test = T.Compose([\n",
    "            T.Resize((265, 128)),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "    def __getitem__(self, index):    # todo\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __len__(self):    # todo\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_train_loader(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Output of this method will be provided to tasks with optimizer in contract\n",
    "        \"\"\"\n",
    "        if self.kwargs['train_bs']:\n",
    "            batch_size = self.kwargs['train_bs']\n",
    "        else:\n",
    "            batch_size = 64\n",
    "\n",
    "        return DataLoader(\n",
    "            ImageDataset(self.dataset.train, transform=self.transform_train),\n",
    "            sampler=RandomIdentitySampler(self.dataset.train, num_instances=4),\n",
    "            batch_size=batch_size, num_workers=4, pin_memory=True, drop_last=True\n",
    "        )\n",
    "\n",
    "    def get_query_loader(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Output of this method will be provided to tasks without optimizer in contract\n",
    "        \"\"\"\n",
    "        if self.kwargs['valid_bs']:\n",
    "            batch_size = self.kwargs['valid_bs']\n",
    "        else:\n",
    "            batch_size = 512\n",
    "\n",
    "        return DataLoader(\n",
    "            ImageDataset(self.dataset.train, transform=self.transform_test),\n",
    "            batch_size=batch_size, num_workers=4, pin_memory=True,\n",
    "            drop_last=False, shuffle=False\n",
    "        )\n",
    "\n",
    "    def get_gallery_loader(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Output of this method will be provided to tasks without optimizer in contract\n",
    "        \"\"\"\n",
    "        if self.kwargs['valid_bs']:\n",
    "            batch_size = self.kwargs['valid_bs']\n",
    "        else:\n",
    "            batch_size = 512\n",
    "\n",
    "        return DataLoader(\n",
    "            ImageDataset(self.dataset.gallery, transform=self.transform_test),\n",
    "            batch_size=batch_size, num_workers=4, pin_memory=True,\n",
    "            drop_last=False, shuffle=False\n",
    "        )\n",
    "\n",
    "    def get_train_data_size(self):\n",
    "        \"\"\"\n",
    "        Information for aggregation\n",
    "        \"\"\"\n",
    "        return self.dataset.num_train_pids\n",
    "\n",
    "    def get_valid_data_size(self):\n",
    "        \"\"\"\n",
    "        Information for aggregation\n",
    "        \"\"\"\n",
    "        return self.dataset.num_gallery_pids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8df35f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fed_dataset = MarketSD(train_bs=4, valid_bs=8)\n",
    "fed_dataset.shard_descriptor = dummy_shard_desc\n",
    "for i, (sample, target) in enumerate(fed_dataset.get_train_loader()):\n",
    "    print(sample.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caring-distinction",
   "metadata": {},
   "source": [
    "### Describe a model and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visible-victor",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foreign-gospel",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ResNet and Classifier definition\n",
    "\"\"\"\n",
    "\n",
    "class ResNet50(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        resnet50 = torchvision.models.resnet50(pretrained=True)\n",
    "        resnet50.layer4[0].conv2.stride = (1, 1)\n",
    "        resnet50.layer4[0].downsample[0].stride = (1, 1)\n",
    "        self.base = nn.Sequential(*list(resnet50.children())[:-2])\n",
    "\n",
    "        self.bn = nn.BatchNorm1d(2048)\n",
    "        nn.init.normal_(self.bn.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(self.bn.bias.data, 0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base(x)\n",
    "        x = nn.functional.avg_pool2d(x, x.size()[2:])\n",
    "        x = x.view(x.size(0), -1)\n",
    "        f = self.bn(x)\n",
    "\n",
    "        return f\n",
    "\n",
    "\n",
    "class NormalizedClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.Tensor(1501, 2048))\n",
    "        self.weight.data.uniform_(-1, 1).renorm_(2,0,1e-5).mul_(1e5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        w = self.weight\n",
    "\n",
    "        x = nn.functional.normalize(x, p=2, dim=1)\n",
    "        w = nn.functional.normalize(w, p=2, dim=1)\n",
    "\n",
    "        return nn.functional.linear(x, w)\n",
    "\n",
    "\n",
    "resnet = ResNet50()\n",
    "classifier = NormalizedClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greater-activation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from losses import ArcFaceLoss, TripletLoss\n",
    "\n",
    "parameters = list(resnet.parameters()) + list(classifier.parameters())\n",
    "optimizer_adam = optim.Adam(parameters, lr=1e-4)\n",
    "\n",
    "criterion_cla = ArcFaceLoss(scale=16., margin=0.1)\n",
    "criterion_pair = TripletLoss(margin=0.3, distance='cosine')\n",
    "\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer_adam, milestones=[20, 40], gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caroline-passion",
   "metadata": {},
   "source": [
    "#### Register model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handled-teens",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "framework_adapter = 'openfl.plugins.frameworks_adapters.pytorch_adapter.FrameworkAdapterPlugin'\n",
    "MI = ModelInterface(model=resnet, optimizer=optimizer_adam, framework_plugin=framework_adapter)    # todo\n",
    "\n",
    "# Save the initial model state\n",
    "initial_model = deepcopy(resnet)    # todo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portuguese-groove",
   "metadata": {},
   "source": [
    "### Define and register FL tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "increasing-builder",
   "metadata": {},
   "outputs": [],
   "source": [
    "TI = TaskInterface()\n",
    "import torch\n",
    "\n",
    "import tqdm\n",
    "\n",
    "from tools import AverageMeter, evaluate, extract_feature\n",
    "\n",
    "\n",
    "# Task interface currently supports only standalone functions.\n",
    "@TI.add_kwargs(**{\n",
    "    'classifier': classifier,\n",
    "    'criterion_cla': criterion_cla,\n",
    "    'criterion_pair': criterion_pair,\n",
    "})\n",
    "@TI.register_fl_task(model='resnet', data_loader='train_loader',\n",
    "                     device='device', optimizer='optimizer')     \n",
    "def train(model, classifier, train_loader, optimizer, device, criterion_cla, criterion_pair):\n",
    "    # if not torch.cuda.is_available():\n",
    "    #     device = 'cpu'\n",
    "    # else:\n",
    "    #     device = 'cuda'\n",
    "    \n",
    "    # function_defined_in_notebook(some_parameter)\n",
    "\n",
    "    batch_cla_loss = AverageMeter()\n",
    "    batch_pair_loss = AverageMeter()\n",
    "    corrects = AverageMeter()\n",
    "\n",
    "    trainloader = tqdm.tqdm(train_loader, desc='train')\n",
    "    \n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    classifier.train()\n",
    "    classifier.to(device)\n",
    "\n",
    "    for batch_idx, (imgs, pids, _) in enumerate(trainloader):\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Forward\n",
    "        features = model(imgs)\n",
    "        outputs = classifier(features)\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "        # Compute loss\n",
    "        cla_loss = criterion_cla(outputs, pids)\n",
    "        pair_loss = criterion_pair(features, pids)\n",
    "        loss = cla_loss + pair_loss\n",
    "        # Backward + Optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # statistics\n",
    "        corrects.update(torch.sum(preds == pids.data).float()/pids.size(0), pids.size(0))\n",
    "        batch_cla_loss.update(cla_loss.item(), pids.size(0))\n",
    "        batch_pair_loss.update(pair_loss.item(), pids.size(0))\n",
    "\n",
    "    return {'ClaLoss': batch_cla_loss.avg,\n",
    "            'PairLoss': batch_pair_loss.avg,\n",
    "            'Accuracy': corrects.avg}\n",
    "\n",
    "\n",
    "@TI.register_fl_task(model='resnet', data_loader='queryloader', device='device')\n",
    "def validate(model, queryloader, galleryloader, device):\n",
    "    # if not torch.cuda.is_available():\n",
    "    #     device = 'cpu'\n",
    "    # else:\n",
    "    #     device = 'cuda'\n",
    "    #\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    # Extract features for query set\n",
    "    qf, q_pids, q_camids = extract_feature(model, queryloader)\n",
    "    print(f'Extracted features for query set, obtained {qf.shape} matrix')\n",
    "    # Extract features for gallery set\n",
    "    gf, g_pids, g_camids = extract_feature(model, galleryloader)\n",
    "    print(f'Extracted features for gallery set, obtained {gf.shape} matrix')\n",
    "    # Compute distance matrix between query and gallery\n",
    "    m, n = qf.size(0), gf.size(0)\n",
    "    distmat = torch.zeros((m,n))\n",
    "    # Cosine similarity\n",
    "    qf = nn.functional.normalize(qf, p=2, dim=1)\n",
    "    gf = nn.functional.normalize(gf, p=2, dim=1)\n",
    "    for i in range(m):\n",
    "        distmat[i] = - torch.mm(qf[i:i+1], gf.t())\n",
    "    distmat = distmat.numpy()\n",
    "\n",
    "    cmc, mAP = evaluate(distmat, q_pids, g_pids, q_camids, g_camids)\n",
    "    return {'top1': cmc[0], 'top5': cmc[4], 'top10': cmc[9], 'mAP': mAP}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "derived-bride",
   "metadata": {},
   "source": [
    "## Time to start a federated learning experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mature-renewal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an experimnet in federation\n",
    "experiment_name = 'market_test_experiment'\n",
    "fl_experiment = FLExperiment(federation=federation, experiment_name=experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lightweight-causing",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# If I use autoreload I got a pickling error\n",
    "\n",
    "# The following command zips the workspace and python requirements to be transfered to collaborator nodes\n",
    "fl_experiment.start(model_provider=MI, \n",
    "                    task_keeper=TI,\n",
    "                    data_loader=fed_dataset,\n",
    "                    rounds_to_train=2,\n",
    "                    opt_treatment='CONTINUE_GLOBAL')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1543a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If user want to stop IPython session, then reconnect and check how experiment is going \n",
    "# fl_experiment.restore_experiment_state(MI)\n",
    "\n",
    "fl_experiment.stream_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c30b301",
   "metadata": {},
   "source": [
    "## Now we validate the best model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55acff59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_model = fl_experiment.get_best_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9479fb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We remove exremove_experiment_datamove_experiment_datamove_experiment_datariment data from director\n",
    "fl_experiment.remove_experiment_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c8aeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.inc.conv[0].weight\n",
    "# model_unet.inc.conv[0].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2acb7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validating initial model\n",
    "validate(initial_model, fed_dataset.get_valid_loader(), 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12ca93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validating trained model\n",
    "validate(best_model, fed_dataset.get_valid_loader(), 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6734f6",
   "metadata": {},
   "source": [
    "## We can tune model further!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3940e75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MI = ModelInterface(model=best_model, optimizer=optimizer_adam, framework_plugin=framework_adapter)\n",
    "fl_experiment.start_experiment(model_provider=MI, task_keeper=TI, data_loader=fed_dataset, rounds_to_train=4, \\\n",
    "                              opt_treatment='CONTINUE_GLOBAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd786d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = fl_experiment.get_best_model()\n",
    "# Validating trained model\n",
    "validate(best_model, fed_dataset.get_valid_loader(), 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcdbfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = (np.zeros((2,4)), np.ones(2,), 2*np.ones(2,))\n",
    "a = [elem for elem in a if elem.shape==2 else elem.newaxis()]\n",
    "np.concatenate(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70863215",
   "metadata": {},
   "outputs": [],
   "source": [
    "class A:\n",
    "    def __init__(self, **kwargs):\n",
    "        print(\"Class A\", kwargs)\n",
    "        \n",
    "class B:\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        print(\"Class B\", kwargs)\n",
    "        \n",
    "class C(B, A):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        print(\"Class C\", kwargs)\n",
    "        \n",
    "# class A:\n",
    "#     def __init__(self, **kwargs):\n",
    "#         print(\"Class A\", kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba44919",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = C(x=1, z=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0637f26f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00ff26c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}